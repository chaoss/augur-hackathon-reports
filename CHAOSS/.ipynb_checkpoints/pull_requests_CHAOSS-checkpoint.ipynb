{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull Request Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Limitations for Reporting on Several Repos\n",
    "The visualizations in this notebook are, like most, able to coherently display information for between 1 and 8 different repositories simultaneously. \n",
    "\n",
    "## Alternatives for Reporting on Repo Groups, Comprising Many Repos\n",
    "The included queries could be rewritten to show an entire repository group's characteristics of that is your primary aim. Specifically, any query could replace this line: \n",
    "```\n",
    "                            WHERE repo.repo_id = {repo_id}\n",
    "```\n",
    "\n",
    "with this line to accomplish the goal of comparing different groups of repositories: \n",
    "```\n",
    "                            WHERE repogroups.repo_group_id = {repo_id}\n",
    "```\n",
    "\n",
    "Simply replace the set of id's in the **Pull Request Filter** section with a list of repo_group_id numbers as well, to accomplish this view. \n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to Your Augur Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd \n",
    "import sqlalchemy as salc\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import datetime\n",
    "import json\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "with open(\"config.json\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "database_connection_string = 'postgres+psycopg2://{}:{}@{}:{}/{}'.format(config['user'], config['password'], config['host'], config['port'], config['database'])\n",
    "\n",
    "dbschema='augur_data'\n",
    "engine = salc.create_engine(\n",
    "    database_connection_string,\n",
    "    connect_args={'options': '-csearch_path={}'.format(dbschema)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Request Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## List of repository IDs for the report\n",
    "repo_set = {25440, 25448}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying the Longest Running Pull Requests\n",
    "\n",
    "## Getting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_all = pd.DataFrame()\n",
    "\n",
    "for repo_id in repo_set: \n",
    "\n",
    "    pr_query = salc.sql.text(f\"\"\"\n",
    "                    SELECT\n",
    "                        repo.repo_id AS repo_id,\n",
    "                        pull_requests.pr_src_id AS pr_src_id,\n",
    "                        repo.repo_name AS repo_name,\n",
    "                        pr_src_author_association,\n",
    "                        repo_groups.rg_name AS repo_group,\n",
    "                        pull_requests.pr_src_state,\n",
    "                        pull_requests.pr_merged_at,\n",
    "                        pull_requests.pr_created_at AS pr_created_at,\n",
    "                        pull_requests.pr_closed_at AS pr_closed_at,\n",
    "                        date_part( 'year', pr_created_at :: DATE ) AS CREATED_YEAR,\n",
    "                        date_part( 'month', pr_created_at :: DATE ) AS CREATED_MONTH,\n",
    "                        date_part( 'year', pr_closed_at :: DATE ) AS CLOSED_YEAR,\n",
    "                        date_part( 'month', pr_closed_at :: DATE ) AS CLOSED_MONTH,\n",
    "                        pr_src_meta_label,\n",
    "                        pr_head_or_base,\n",
    "                        ( EXTRACT ( EPOCH FROM pull_requests.pr_closed_at ) - EXTRACT ( EPOCH FROM pull_requests.pr_created_at ) ) / 3600 AS hours_to_close,\n",
    "                        ( EXTRACT ( EPOCH FROM pull_requests.pr_closed_at ) - EXTRACT ( EPOCH FROM pull_requests.pr_created_at ) ) / 86400 AS days_to_close, \n",
    "                        ( EXTRACT ( EPOCH FROM first_response_time ) - EXTRACT ( EPOCH FROM pull_requests.pr_created_at ) ) / 3600 AS hours_to_first_response,\n",
    "                        ( EXTRACT ( EPOCH FROM first_response_time ) - EXTRACT ( EPOCH FROM pull_requests.pr_created_at ) ) / 86400 AS days_to_first_response, \n",
    "                        ( EXTRACT ( EPOCH FROM last_response_time ) - EXTRACT ( EPOCH FROM pull_requests.pr_created_at ) ) / 3600 AS hours_to_last_response,\n",
    "                        ( EXTRACT ( EPOCH FROM last_response_time ) - EXTRACT ( EPOCH FROM pull_requests.pr_created_at ) ) / 86400 AS days_to_last_response, \n",
    "                        first_response_time,\n",
    "                        last_response_time,\n",
    "                        average_time_between_responses,\n",
    "                        assigned_count,\n",
    "                        review_requested_count,\n",
    "                        labeled_count,\n",
    "                        subscribed_count,\n",
    "                        mentioned_count,\n",
    "                        referenced_count,\n",
    "                        closed_count,\n",
    "                        head_ref_force_pushed_count,\n",
    "                        merged_count,\n",
    "                        milestoned_count,\n",
    "                        unlabeled_count,\n",
    "                        head_ref_deleted_count,\n",
    "                        comment_count,\n",
    "                        lines_added, \n",
    "                        lines_removed,\n",
    "                        commit_count, \n",
    "                        file_count\n",
    "                    FROM\n",
    "                        repo,\n",
    "                        repo_groups,\n",
    "                        pull_requests LEFT OUTER JOIN ( \n",
    "                            SELECT pull_requests.pull_request_id,\n",
    "                            count(*) FILTER (WHERE action = 'assigned') AS assigned_count,\n",
    "                            count(*) FILTER (WHERE action = 'review_requested') AS review_requested_count,\n",
    "                            count(*) FILTER (WHERE action = 'labeled') AS labeled_count,\n",
    "                            count(*) FILTER (WHERE action = 'unlabeled') AS unlabeled_count,\n",
    "                            count(*) FILTER (WHERE action = 'subscribed') AS subscribed_count,\n",
    "                            count(*) FILTER (WHERE action = 'mentioned') AS mentioned_count,\n",
    "                            count(*) FILTER (WHERE action = 'referenced') AS referenced_count,\n",
    "                            count(*) FILTER (WHERE action = 'closed') AS closed_count,\n",
    "                            count(*) FILTER (WHERE action = 'head_ref_force_pushed') AS head_ref_force_pushed_count,\n",
    "                            count(*) FILTER (WHERE action = 'head_ref_deleted') AS head_ref_deleted_count,\n",
    "                            count(*) FILTER (WHERE action = 'milestoned') AS milestoned_count,\n",
    "                            count(*) FILTER (WHERE action = 'merged') AS merged_count,\n",
    "                            MIN(message.msg_timestamp) AS first_response_time,\n",
    "                            COUNT(DISTINCT message.msg_timestamp) AS comment_count,\n",
    "                            MAX(message.msg_timestamp) AS last_response_time,\n",
    "                            (MAX(message.msg_timestamp) - MIN(message.msg_timestamp)) / COUNT(DISTINCT message.msg_timestamp) AS average_time_between_responses\n",
    "                            FROM pull_request_events, pull_requests, repo, pull_request_message_ref, message\n",
    "                            WHERE repo.repo_id = {repo_id}\n",
    "                            AND repo.repo_id = pull_requests.repo_id\n",
    "                            AND pull_requests.pull_request_id = pull_request_events.pull_request_id\n",
    "                            AND pull_requests.pull_request_id = pull_request_message_ref.pull_request_id\n",
    "                            AND pull_request_message_ref.msg_id = message.msg_id\n",
    "                            GROUP BY pull_requests.pull_request_id\n",
    "                        ) response_times\n",
    "                        ON pull_requests.pull_request_id = response_times.pull_request_id\n",
    "                        LEFT OUTER JOIN (\n",
    "                            SELECT pull_request_commits.pull_request_id, count(DISTINCT pr_cmt_sha) AS commit_count                                FROM pull_request_commits, pull_requests, pull_request_meta\n",
    "                            WHERE pull_requests.pull_request_id = pull_request_commits.pull_request_id\n",
    "                            AND pull_requests.pull_request_id = pull_request_meta.pull_request_id\n",
    "                            AND pull_requests.repo_id = {repo_id}\n",
    "                            AND pr_cmt_sha <> pull_requests.pr_merge_commit_sha\n",
    "                            AND pr_cmt_sha <> pull_request_meta.pr_sha\n",
    "                            GROUP BY pull_request_commits.pull_request_id\n",
    "                        ) all_commit_counts\n",
    "                        ON pull_requests.pull_request_id = all_commit_counts.pull_request_id\n",
    "                        LEFT OUTER JOIN (\n",
    "                            SELECT MAX(pr_repo_meta_id), pull_request_meta.pull_request_id, pr_head_or_base, pr_src_meta_label\n",
    "                            FROM pull_requests, pull_request_meta\n",
    "                            WHERE pull_requests.pull_request_id = pull_request_meta.pull_request_id\n",
    "                            AND pull_requests.repo_id = {repo_id}\n",
    "                            AND pr_head_or_base = 'base'\n",
    "                            GROUP BY pull_request_meta.pull_request_id, pr_head_or_base, pr_src_meta_label\n",
    "                        ) base_labels\n",
    "                        ON base_labels.pull_request_id = all_commit_counts.pull_request_id\n",
    "                        LEFT OUTER JOIN (\n",
    "                            SELECT sum(cmt_added) AS lines_added, sum(cmt_removed) AS lines_removed, pull_request_commits.pull_request_id, count(DISTINCT cmt_filename) AS file_count\n",
    "                            FROM pull_request_commits, commits, pull_requests, pull_request_meta\n",
    "                            WHERE cmt_commit_hash = pr_cmt_sha\n",
    "                            AND pull_requests.pull_request_id = pull_request_commits.pull_request_id\n",
    "                            AND pull_requests.pull_request_id = pull_request_meta.pull_request_id\n",
    "                            AND pull_requests.repo_id = {repo_id}\n",
    "                            AND commits.repo_id = pull_requests.repo_id\n",
    "                            AND commits.cmt_commit_hash <> pull_requests.pr_merge_commit_sha\n",
    "                            AND commits.cmt_commit_hash <> pull_request_meta.pr_sha\n",
    "                            GROUP BY pull_request_commits.pull_request_id\n",
    "                        ) master_merged_counts \n",
    "                        ON base_labels.pull_request_id = master_merged_counts.pull_request_id                    \n",
    "                    WHERE \n",
    "                        repo.repo_group_id = repo_groups.repo_group_id \n",
    "                        AND repo.repo_id = pull_requests.repo_id \n",
    "                        AND repo.repo_id = {repo_id} \n",
    "                    ORDER BY\n",
    "                       merged_count DESC\n",
    "        \"\"\")\n",
    "    pr_a = pd.read_sql(pr_query, con=engine)\n",
    "    if not pr_all.empty: \n",
    "        pr_all = pd.concat([pr_all, pr_a]) \n",
    "    else: \n",
    "        # first repo\n",
    "        pr_all = pr_a\n",
    "display(pr_all.head())\n",
    "pr_all.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin data pre-processing and adding columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data type changing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change count columns from float datatype to integer\n",
    "pr_all[['assigned_count',\n",
    "          'review_requested_count',\n",
    "          'labeled_count',\n",
    "          'subscribed_count',\n",
    "          'mentioned_count',\n",
    "          'referenced_count',\n",
    "          'closed_count',\n",
    "          'head_ref_force_pushed_count',\n",
    "          'merged_count',\n",
    "          'milestoned_count',\n",
    "          'unlabeled_count',\n",
    "          'head_ref_deleted_count',\n",
    "          'comment_count',\n",
    "        'commit_count',\n",
    "        'file_count',\n",
    "        'lines_added',\n",
    "        'lines_removed'\n",
    "       ]] = pr_all[['assigned_count',\n",
    "                                      'review_requested_count',\n",
    "                                      'labeled_count',\n",
    "                                      'subscribed_count',\n",
    "                                      'mentioned_count',\n",
    "                                      'referenced_count',\n",
    "                                      'closed_count',\n",
    "                                        'head_ref_force_pushed_count',\n",
    "                                    'merged_count',\n",
    "                                      'milestoned_count',          \n",
    "                                      'unlabeled_count',\n",
    "                                      'head_ref_deleted_count',\n",
    "                                      'comment_count',\n",
    "                                        'commit_count',\n",
    "                                        'file_count',\n",
    "                                        'lines_added',\n",
    "                                        'lines_removed'\n",
    "                   ]].astype(float)\n",
    "# Change years to int so that doesn't display as 2019.0 for example\n",
    "pr_all[[\n",
    "            'created_year',\n",
    "           'closed_year']] = pr_all[['created_year',\n",
    "                                       'closed_year']].fillna(-1).astype(int).astype(str)\n",
    "pr_all.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pr_all['repo_name'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add `average_days_between_responses` and `average_hours_between_responses` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get days for average_time_between_responses time delta\n",
    "\n",
    "pr_all['average_days_between_responses'] = pr_all['average_time_between_responses'].map(lambda x: x.days).astype(float)\n",
    "pr_all['average_hours_between_responses'] = pr_all['average_time_between_responses'].map(lambda x: x.days * 24).astype(float)\n",
    "\n",
    "pr_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date filtering entire dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = pd.to_datetime('2017-07-01 01:00:00')\n",
    "# end_date = pd.to_datetime('2020-02-01 09:00:00')\n",
    "end_date = pd.to_datetime('2019-12-31 23:59:59')\n",
    "pr_all = pr_all[(pr_all['pr_created_at'] > start_date) & (pr_all['pr_closed_at'] < end_date)]\n",
    "\n",
    "pr_all['created_year'] = pr_all['created_year'].map(int)\n",
    "pr_all['created_month'] = pr_all['created_month'].map(int)\n",
    "pr_all['created_month'] = pr_all['created_month'].map(lambda x: '{0:0>2}'.format(x))\n",
    "pr_all['created_yearmonth'] = pd.to_datetime(pr_all['created_year'].map(str) + '-' + pr_all['created_month'].map(str) + '-01')\n",
    "pr_all.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add `days_to_close` column for pull requests that are still open (closed pull requests already have this column filled from the query)\n",
    "\n",
    "Note: there will be no pull requests that are still open in the dataframe if you filtered by an end date in the above cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# getting the number of days of (today - created at) for the PRs that are still open\n",
    "# and putting this in the days_to_close column\n",
    "\n",
    "# get timedeltas of creation time to todays date/time\n",
    "days_to_close_open_pr = datetime.datetime.now() - pr_all.loc[pr_all['pr_src_state'] == 'open']['pr_created_at']\n",
    "\n",
    "# get num days from above timedelta\n",
    "days_to_close_open_pr = days_to_close_open_pr.apply(lambda x: x.days).astype(int)\n",
    "\n",
    "# for only OPEN pr's, set the days_to_close column equal to above dataframe\n",
    "pr_all.loc[pr_all['pr_src_state'] == 'open'] = pr_all.loc[pr_all['pr_src_state'] == 'open'].assign(days_to_close=days_to_close_open_pr)\n",
    "\n",
    "pr_all.loc[pr_all['pr_src_state'] == 'open'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add `closed_yearmonth` column for only CLOSED pull requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate column by setting all null datetimes\n",
    "pr_all['closed_yearmonth'] = pd.to_datetime(np.nan)\n",
    "\n",
    "# Fill column with prettified string of year/month closed that looks like: 2019-07-01\n",
    "pr_all.loc[pr_all['pr_src_state'] == 'closed'] = pr_all.loc[pr_all['pr_src_state'] == 'closed'].assign(\n",
    "    closed_yearmonth = pd.to_datetime(pr_all.loc[pr_all['pr_src_state'] == 'closed']['closed_year'].astype(int\n",
    "        ).map(str) + '-' + pr_all.loc[pr_all['pr_src_state'] == 'closed']['closed_month'].astype(int).map(str) + '-01'))\n",
    "\n",
    "pr_all.loc[pr_all['pr_src_state'] == 'closed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add `merged_flag` column which is just prettified strings based off of if the `pr_merged_at` column is null or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Merged flag \"\"\"\n",
    "if 'pr_merged_at' in pr_all.columns.values:\n",
    "    pr_all['pr_merged_at'] = pr_all['pr_merged_at'].fillna(0)\n",
    "    pr_all['merged_flag'] = 'Not Merged / Rejected'\n",
    "    pr_all['merged_flag'].loc[pr_all['pr_merged_at'] != 0] = 'Merged / Accepted'\n",
    "    pr_all['merged_flag'].loc[pr_all['pr_src_state'] == 'open'] = 'Still Open'\n",
    "    del pr_all['pr_merged_at']\n",
    "pr_all['merged_flag']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into different dataframes\n",
    "### All, open, closed, and slowest 20% of these 3 categories (6 dataframes total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate the different state PRs for now\n",
    "pr_open = pr_all.loc[pr_all['pr_src_state'] == 'open']\n",
    "pr_closed = pr_all.loc[pr_all['pr_src_state'] == 'closed']\n",
    "pr_merged = pr_all.loc[pr_all['merged_flag'] == 'Merged / Accepted']\n",
    "pr_not_merged = pr_all.loc[pr_all['merged_flag'] == 'Not Merged / Rejected']\n",
    "pr_closed['merged_flag']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframes that contain the slowest 20% pull requests of each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the 80th percentile slowest PRs\n",
    "\n",
    "def filter_20_per_slowest(input_df):\n",
    "    pr_slow20_filtered = pd.DataFrame()\n",
    "    pr_slow20_x = pd.DataFrame()\n",
    "    for value in repo_set: \n",
    "        if not pr_slow20_filtered.empty: \n",
    "            pr_slow20x = input_df.query('repo_id==@value')\n",
    "            pr_slow20x['percentile_rank_local'] = pr_slow20x.days_to_close.rank(pct=True)\n",
    "            pr_slow20x = pr_slow20x.query('percentile_rank_local >= .8', )\n",
    "            pr_slow20_filtered = pd.concat([pr_slow20x, pr_slow20_filtered]) \n",
    "            reponame = str(value)\n",
    "            filename = ''.join(['output/pr_slowest20pct', reponame, '.csv'])\n",
    "            pr_slow20x.to_csv(filename)\n",
    "        else: \n",
    "            # first time\n",
    "            pr_slow20_filtered = input_df.copy()\n",
    "            pr_slow20_filtered['percentile_rank_local'] = pr_slow20_filtered.days_to_close.rank(pct=True)\n",
    "            pr_slow20_filtered = pr_slow20_filtered.query('percentile_rank_local >= .8', )\n",
    "#     print(pr_slow20_filtered.describe())\n",
    "    return pr_slow20_filtered\n",
    "\n",
    "pr_slow20_open = filter_20_per_slowest(pr_open)\n",
    "pr_slow20_closed = filter_20_per_slowest(pr_closed)\n",
    "pr_slow20_merged = filter_20_per_slowest(pr_merged)\n",
    "pr_slow20_not_merged = filter_20_per_slowest(pr_not_merged)\n",
    "pr_slow20_all = filter_20_per_slowest(pr_all)\n",
    "pr_slow20_merged#.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Visualization Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.palettes import Colorblind, mpl, magma, Accent, GnBu3, OrRd3, Category20, inferno, Plasma256, Turbo256\n",
    "import bokeh\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.models.annotations import Title\n",
    "from bokeh.io import export_png\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.models import ColumnDataSource, Legend, LabelSet, Range1d, LinearAxis\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.sampledata.sprint import sprint\n",
    "from bokeh.models.glyphs import Rect\n",
    "from bokeh.transform import dodge\n",
    "\n",
    "try:\n",
    "    colors = Colorblind[len(repo_set)]\n",
    "except:\n",
    "    colors = Colorblind[3]\n",
    "#mpl['Plasma'][len(repo_set)]\n",
    "#['A6CEE3','B2DF8A','33A02C','FB9A99']\n",
    "\n",
    "def remove_outliers(input_df, field, num_outliers_repo_map):\n",
    "    df_no_outliers = input_df.copy()\n",
    "    for repo_name, num_outliers in num_outliers_repo_map.items():\n",
    "        indices_to_drop = input_df.loc[input_df['repo_name'] == repo_name].nlargest(num_outliers, field).index\n",
    "        df_no_outliers = df_no_outliers.drop(index=indices_to_drop)\n",
    "    return df_no_outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import DateFormatter\n",
    "import datetime as dt\n",
    "\n",
    "def visualize_mean_days_to_close(input_df, x_axis='closed_yearmonth', description='Closed', save_file=False, num_remove_outliers=0, drop_outliers_repo=None):\n",
    "\n",
    "    # Set the df you want to build the viz's for\n",
    "    driver_df = input_df.copy()\n",
    "    \n",
    "    driver_df = driver_df[['repo_id', 'repo_name', 'pr_src_id', 'created_yearmonth', 'closed_yearmonth', 'days_to_close']]\n",
    "\n",
    "    if save_file:\n",
    "        driver_df.to_csv('output/c.pr_slow20small {}.csv'.format(description))\n",
    "    \n",
    "    driver_df_mean = driver_df.groupby(['repo_id', x_axis, 'repo_name'],as_index=False).mean()\n",
    "        \n",
    "    # Total PRS Closed\n",
    "    fig, ax = plt.subplots()\n",
    "    # the size of A4 paper\n",
    "    fig.set_size_inches(16, 8)\n",
    "    plotter = sns.lineplot(x=x_axis, y='days_to_close', style='repo_name', data=driver_df_mean, sort=True, legend='full', linewidth=2.5, hue='repo_name').set_title(\"Average Days to Close of {} Pull Requests, July 2017-January 2020\".format(description))  \n",
    "    if save_file:\n",
    "        fig.savefig('images/slow_20_mean {}.png'.format(description))\n",
    "    \n",
    "    # Copying array and deleting the outlier in the copy to re-visualize\n",
    "    def drop_n_largest(input_df, n, repo_name):\n",
    "        input_df_copy = input_df.copy()\n",
    "        indices_to_drop = input_df.loc[input_df['repo_name'] == 'amazon-freertos'].nlargest(n,'days_to_close').index\n",
    "        print(\"Indices to drop: {}\".format(indices_to_drop))\n",
    "        input_df_copy = input_df_copy.drop(index=indices_to_drop)\n",
    "        input_df_copy.loc[input_df['repo_name'] == repo_name]\n",
    "        return input_df_copy\n",
    "\n",
    "    if num_remove_outliers > 0 and drop_outliers_repo:\n",
    "        driver_df_mean_no_outliers = drop_n_largest(driver_df_mean, num_remove_outliers, drop_outliers_repo)\n",
    "    \n",
    "        # Total PRS Closed without outlier\n",
    "        fig, ax = plt.subplots()\n",
    "        # the size of A4 paper\n",
    "        fig.set_size_inches(16, 8)\n",
    "        plotter = sns.lineplot(x=x_axis, y='days_to_close', style='repo_name', data=driver_df_mean_no_outliers, sort=False, legend='full', linewidth=2.5, hue='repo_name').set_title(\"Average Days to Close among {} Pull Requests Without Outlier, July 2017-January 2020\".format(description))\n",
    "        plotterlabels = ax.set_xticklabels(driver_df_mean_no_outliers[x_axis], rotation=90, fontsize=8)\n",
    "        if save_file:\n",
    "            fig.savefig('images/slow_20_mean_no_outlier {}.png'.format(description))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_mean_days_to_close(pr_closed, description='All Closed', save_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import ColumnDataSource, FactorRange\n",
    "from bokeh.transform import factor_cmap\n",
    "\n",
    "def vertical_grouped_bar(input_df, group_by = 'merged_flag', x_axis='closed_year', y_axis='num_commits', y_max=None, repo_name=None, description=\"\", title =\"\", save_file=False):\n",
    "    output_notebook() # let bokeh display plot in jupyter cell output\n",
    "    \n",
    "    driver_df = input_df.copy() # deep copy input data so we do not change the external dataframe \n",
    "    \n",
    "    # Filter df by passed *repo_name* param\n",
    "    if repo_name:\n",
    "        driver_df = driver_df.loc[driver_df['repo_name'] == repo_name]\n",
    "        \n",
    "    # Change closed year to int so that doesn't display as 2019.0 for example\n",
    "    driver_df['closed_year'] = driver_df['closed_year'].astype(int).astype(str)\n",
    "    \n",
    "    # contains the closed years\n",
    "    x_groups = sorted(list(driver_df[x_axis].unique()))\n",
    "    \n",
    "    # inner groups on x_axis they are merged and not_merged\n",
    "    groups = list(driver_df[group_by].unique())\n",
    "    \n",
    "    # setup color pallete\n",
    "    try:\n",
    "        colors = mpl['Plasma'][len(groups)]\n",
    "    except:\n",
    "        colors = [mpl['Plasma'][3][0]] + [mpl['Plasma'][3][1]]\n",
    "        \n",
    "    merged_avg_values = list(driver_df.loc[driver_df[group_by] == 'Merged / Accepted'].groupby([x_axis],as_index=False).mean().round(1)['commit_count'])\n",
    "    not_merged_avg_values = list(driver_df.loc[driver_df[group_by] == 'Not Merged / Rejected'].groupby([x_axis],as_index=False).mean().round(1)['commit_count'])\n",
    "        \n",
    "    # Setup data in format for grouped bar chart\n",
    "    data = {\n",
    "            'years' : x_groups,\n",
    "            'Merged / Accepted'       : merged_avg_values,\n",
    "            'Not Merged / Rejected'   : not_merged_avg_values,\n",
    "        }\n",
    "    \n",
    "    x = [ (year, pr_state) for year in x_groups for pr_state in groups ]\n",
    "    counts = sum(zip(data['Merged / Accepted'], data['Not Merged / Rejected']), ())\n",
    "    \n",
    "    source = ColumnDataSource(data=dict(x=x, counts=counts))\n",
    "\n",
    "    title_beginning = '{}: '.format(repo_name) if repo_name else ''\n",
    "    if y_max:\n",
    "        p = figure(x_range=FactorRange(*x), plot_height=450, plot_width=700, title=title.format(title_beginning, description), y_range=(0,y_max))\n",
    "    else:\n",
    "        p = figure(x_range=FactorRange(*x), plot_height=450, plot_width=700, title=title.format(title_beginning, description))\n",
    "\n",
    "    # Vertical bar glyph\n",
    "    p.vbar(x='x', top='counts', width=0.9, source=source, line_color=\"white\",\n",
    "           fill_color=factor_cmap('x', palette=colors, factors=groups, start=1, end=2))\n",
    "    \n",
    "    # Data label \n",
    "    labels = LabelSet(x='x', y='counts', text='counts',# y_offset=-8, x_offset=34,\n",
    "              text_font_size=\"12pt\", text_color=\"black\",\n",
    "              source=source, text_align='center')\n",
    "    p.add_layout(labels)\n",
    "\n",
    "    p.y_range.start = 0\n",
    "    p.x_range.range_padding = 0.1\n",
    "    p.xaxis.major_label_orientation = 1\n",
    "    p.xgrid.grid_line_color = None\n",
    "    \n",
    "    p.yaxis.axis_label = 'Average Commits / Pull Request'\n",
    "    p.xaxis.axis_label = 'Year Closed'\n",
    "    \n",
    "    p.title.align = \"center\"\n",
    "    p.title.text_font_size = \"16px\"\n",
    "    \n",
    "    p.xaxis.axis_label_text_font_size = \"15px\"\n",
    "    p.xaxis.major_label_text_font_size = \"12px\"\n",
    "    \n",
    "    p.yaxis.axis_label_text_font_size = \"15px\"\n",
    "    p.yaxis.major_label_text_font_size = \"15px\"\n",
    "    \n",
    "    show(p)\n",
    "    \n",
    "    if save_file:\n",
    "        export_png(p, filename=\"./images/v_grouped_bar/v_grouped_bar__{}_PRs__yaxis_{}__repo_{}.png\".format(description, y_axis, repo_name))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical_grouped_bar(pr_all, description='All', title=\"{}Average Commit Counts Per Year for {} Pull Requests\", save_file=False, y_max=8.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vertical_grouped_bar_line_counts(input_df,  x_axis='closed_year', y_max1=600000, y_max2=1000, repo_name=None, description=\"\", title =\"\", save_file=False):\n",
    "    output_notebook() # let bokeh display plot in jupyter cell output\n",
    "    \n",
    "    driver_df = input_df.copy() # deep copy input data so we do not change the external dataframe \n",
    "    \n",
    "    # Filter df by passed *repo_name* param\n",
    "    if repo_name:\n",
    "        driver_df = driver_df.loc[driver_df['repo_name'] == repo_name]\n",
    "        \n",
    "    # Change closed year to int so that doesn't display as 2019.0 for example\n",
    "    driver_df['closed_year'] = driver_df['closed_year'].astype(int).astype(str)\n",
    "    \n",
    "    # contains the closed years\n",
    "    x_groups = sorted(list(driver_df[x_axis].unique()))\n",
    "    \n",
    "    groups = ['Lines Added', 'Lines Removed', 'Files Changed']\n",
    "    \n",
    "    # setup color pallete\n",
    "    colors = mpl['Plasma'][3]\n",
    "    \n",
    "    display(pr_all[pr_all['lines_added'].notna()])#.groupby([x_axis],as_index=False).mean())\n",
    "        \n",
    "    files_avg_values = list(driver_df.groupby([x_axis],as_index=False).mean().round(1)['file_count'])\n",
    "    added_avg_values = list(driver_df.groupby([x_axis],as_index=False).mean().round(1)['lines_added'])\n",
    "    removed_avg_values = list(driver_df.groupby([x_axis],as_index=False).mean().round(1)['lines_removed'])\n",
    "    display(driver_df.groupby([x_axis],as_index=False).mean())\n",
    "    print(files_avg_values)\n",
    "    print(added_avg_values)\n",
    "    print(removed_avg_values)\n",
    "    \n",
    "        \n",
    "    # Setup data in format for grouped bar chart\n",
    "    data = {\n",
    "            'years' : x_groups,\n",
    "            'Lines Added'   : added_avg_values,\n",
    "            'Lines Removed' : removed_avg_values,\n",
    "            'Files Changed' : files_avg_values\n",
    "        }\n",
    "\n",
    "    x = [ (year, pr_state) for year in x_groups for pr_state in groups ]\n",
    "    line_counts = sum(zip(data['Lines Added'], data['Lines Removed'], [0]*len(x_groups)), ())\n",
    "    file_counts = sum(zip([0]*len(x_groups),[0]*len(x_groups),data['Files Changed']), ())\n",
    "    print(line_counts)\n",
    "    print(file_counts)\n",
    "    \n",
    "    source = ColumnDataSource(data=dict(x=x, line_counts=line_counts, file_counts=file_counts))\n",
    "\n",
    "    if y_max1:\n",
    "        p = figure(x_range=FactorRange(*x), plot_height=450, plot_width=700, title=title.format(description), y_range=(0,y_max1))\n",
    "    else:\n",
    "        p = figure(x_range=FactorRange(*x), plot_height=450, plot_width=700, title=title.format(description))\n",
    "                \n",
    "    # Setting the second y axis range name and range\n",
    "    p.extra_y_ranges = {\"file_counts\": Range1d(start=0, end=y_max2)}\n",
    "    \n",
    "    # Adding the second axis to the plot.  \n",
    "    p.add_layout(LinearAxis(y_range_name=\"file_counts\"), 'right')\n",
    "    \n",
    "    # Data label for line counts\n",
    "    labels = LabelSet(x='x', y='line_counts', text='line_counts',y_offset=8,# x_offset=34,\n",
    "              text_font_size=\"10pt\", text_color=\"black\",\n",
    "              source=source, text_align='center')\n",
    "    p.add_layout(labels)\n",
    "\n",
    "    # Vertical bar glyph for line counts\n",
    "    p.vbar(x='x', top='line_counts', width=0.9, source=source, line_color=\"white\",\n",
    "           fill_color=factor_cmap('x', palette=colors, factors=groups, start=1, end=2))\n",
    "    \n",
    "    # Data label for file counts\n",
    "    labels = LabelSet(x='x', y='file_counts', text='file_counts', y_offset=0, #x_offset=34,\n",
    "              text_font_size=\"10pt\", text_color=\"black\",\n",
    "              source=source, text_align='center', y_range_name=\"file_counts\")\n",
    "    p.add_layout(labels)\n",
    "    \n",
    "    # Vertical bar glyph for file counts\n",
    "    p.vbar(x='x', top='file_counts', width=0.9, source=source, line_color=\"white\",\n",
    "           fill_color=factor_cmap('x', palette=colors, factors=groups, start=1, end=2), y_range_name=\"file_counts\")\n",
    "\n",
    "    p.left[0].formatter.use_scientific = False\n",
    "    p.y_range.start = 0\n",
    "    p.x_range.range_padding = 0.1\n",
    "    p.xaxis.major_label_orientation = 1\n",
    "    p.xgrid.grid_line_color = None\n",
    "    \n",
    "    p.yaxis.axis_label = 'Average Commits / Pull Request'\n",
    "    p.xaxis.axis_label = 'Year Closed'\n",
    "    \n",
    "    p.title.align = \"center\"\n",
    "    p.title.text_font_size = \"16px\"\n",
    "    \n",
    "    p.xaxis.axis_label_text_font_size = \"15px\"\n",
    "    p.xaxis.major_label_text_font_size = \"12px\"\n",
    "    \n",
    "    p.yaxis.axis_label_text_font_size = \"15px\"\n",
    "    p.yaxis.major_label_text_font_size = \"15px\"\n",
    "    \n",
    "    show(p)\n",
    "    \n",
    "    if save_file:\n",
    "        export_png(p, filename=\"./images/v_grouped_bar/v_grouped_bar__{}_PRs__yaxis_{}__repo_{}.png\".format(description, y_axis, repo_name))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" THIS VIZ IS NOT READY YET , BUT UNCOMMENT LINE BELOW IF YOU WANT TO SEE\"\"\"\n",
    "# vertical_grouped_bar_line_counts(pr_all, description='All', title=\"Average Size Metrics Per Year for {} Merged Pull Requests in Master\", save_file=False, y_max1=580000, y_max2=1100)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def horizontal_stacked_bar(input_df, group_by='merged_flag', x_axis='comment_count', repo_name=None, x_max=1000, description=\"\", y_axis='repo_name', title=\"Average Pull Request Comments by Merged Status\", save_file=False):\n",
    "    driver_df = input_df.copy()\n",
    "    \n",
    "    if repo_name:\n",
    "        driver_df = driver_df.loc[driver_df['repo_name'] == repo_name]\n",
    "        \n",
    "    output_notebook()\n",
    "    \n",
    "    try:\n",
    "        y_groups = sorted(list(driver_df[y_axis].unique()))\n",
    "    except:\n",
    "        y_groups = [repo_name]\n",
    "        \n",
    "    groups = driver_df[group_by].unique()\n",
    "    try:\n",
    "        colors = mpl['Plasma'][len(groups)]\n",
    "    except:\n",
    "        colors = [mpl['Plasma'][3][0]] + [mpl['Plasma'][3][1]]\n",
    "        \n",
    "    len_not_merged = len(driver_df.loc[driver_df['merged_flag'] == 'Not Merged / Rejected'])\n",
    "    len_merged = len(driver_df.loc[driver_df['merged_flag'] == 'Merged / Accepted'])\n",
    "\n",
    "    title_beginning = '{}: '.format(repo_name) if repo_name else ''\n",
    "    p = figure(y_range=y_groups, plot_height=450, plot_width=650, x_range=(0,x_max), # y_range=y_groups,#(pr_all[y_axis].min(),pr_all[y_axis].max()) #y_axis_type=\"datetime\",\n",
    "               title='{} {}'.format(title_beginning, title.format(description)))\n",
    "\n",
    "    for y_value in y_groups:\n",
    "\n",
    "        y_merged_data = driver_df.loc[(driver_df[y_axis] == y_value) & (driver_df['merged_flag'] == 'Merged / Accepted')]\n",
    "        y_not_merged_data = driver_df.loc[(driver_df[y_axis] == y_value) & (driver_df['merged_flag'] == 'Not Merged / Rejected')]\n",
    "\n",
    "        if len(y_merged_data) > 0:\n",
    "            y_merged_data[x_axis + '_mean'] = y_merged_data[x_axis].mean().round(1)\n",
    "        else:\n",
    "            y_merged_data[x_axis + '_mean'] = 0.00\n",
    "\n",
    "        if len(y_not_merged_data) > 0:\n",
    "            y_not_merged_data[x_axis + '_mean'] = y_not_merged_data[x_axis].mean().round(1)\n",
    "        else:\n",
    "            y_not_merged_data[x_axis + '_mean'] = 0\n",
    "\n",
    "        not_merged_source = ColumnDataSource(y_not_merged_data)\n",
    "        merged_source = ColumnDataSource(y_merged_data)\n",
    "\n",
    "        # mean comment count for merged\n",
    "        merged_comment_count_glyph = p.hbar(y=dodge(y_axis, -0.1, range=p.y_range), left=0, right=x_axis + '_mean', height=0.04*len(driver_df[y_axis].unique()), \n",
    "                                     source=merged_source, fill_color=\"black\")#,legend_label=\"Mean Days to Close\",\n",
    "        # Data label \n",
    "        labels = LabelSet(x=x_axis + '_mean', y=dodge(y_axis, -0.1, range=p.y_range), text=x_axis + '_mean', y_offset=-8, x_offset=34,\n",
    "                  text_font_size=\"12pt\", text_color=\"black\",\n",
    "                  source=merged_source, text_align='center')\n",
    "        p.add_layout(labels)\n",
    "        # mean comment count For nonmerged\n",
    "        not_merged_comment_count_glyph = p.hbar(y=dodge(y_axis, 0.1, range=p.y_range), left=0, right=x_axis + '_mean', \n",
    "                                     height=0.04*len(driver_df[y_axis].unique()), source=not_merged_source, fill_color=\"#e84d60\")#legend_label=\"Mean Days to Close\",\n",
    "        # Data label \n",
    "        labels = LabelSet(x=x_axis + '_mean', y=dodge(y_axis, 0.1, range=p.y_range), text=x_axis + '_mean', y_offset=-8, x_offset=34,\n",
    "                  text_font_size=\"12pt\", text_color=\"#e84d60\",\n",
    "                  source=not_merged_source, text_align='center')\n",
    "        p.add_layout(labels)\n",
    "\n",
    "#         p.y_range.range_padding = 0.1\n",
    "    p.ygrid.grid_line_color = None\n",
    "    p.legend.location = \"bottom_right\"\n",
    "    p.axis.minor_tick_line_color = None\n",
    "    p.outline_line_color = None\n",
    "    p.xaxis.axis_label = 'Average Comments / Pull Request'\n",
    "    p.yaxis.axis_label = 'Repository' if y_axis == 'repo_name' else 'Year Closed' if y_axis == 'closed_year' else ''\n",
    "\n",
    "    legend = Legend(\n",
    "            items=[\n",
    "                (\"Merged Pull Request Mean Comment Count\", [merged_comment_count_glyph]),\n",
    "                (\"Rejected Pull Request Mean Comment Count\", [not_merged_comment_count_glyph])\n",
    "            ],\n",
    "\n",
    "            location='center', \n",
    "            orientation='vertical',\n",
    "            border_line_color=\"black\"\n",
    "        )\n",
    "    p.add_layout(legend, \"below\")\n",
    "            \n",
    "    show(p, plot_width=1200, plot_height=1200)\n",
    "    \n",
    "    if save_file:\n",
    "        repo_extension = 'All' if not repo_name else repo_name\n",
    "        export_png(p, filename=\"./images/h_stacked_bar_mean_comments_merged_status/mean_comments_merged_status__facet_{}__{}_PRs__yaxis_{}__repo_{}.png\".format(facet, description, y_axis, repo_extension))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizontal_stacked_bar(pr_closed, description=\"All Closed\", group_by='merged_flag', save_file=False, x_max=5.8, x_axis='comment_count', y_axis='closed_year', title=\"Mean Comments for {} Pull Requests\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merged_ratio_vertical_grouped_bar(data_dict, repo_name=None, x_axis='repo_name', save_file=False, y_max=5000, description=\"All\", title=\"{}\"):\n",
    "        \n",
    "    output_notebook()\n",
    "        \n",
    "    colors = mpl['Plasma'][6]\n",
    "        \n",
    "    if repo_name == 'mbed-os':\n",
    "        colors = colors[::-1]\n",
    "    \n",
    "    for data_desc, input_df in data_dict.items():\n",
    "        x_groups = sorted(list(input_df[x_axis].astype(str).unique()))\n",
    "        break\n",
    "    \n",
    "    title_beginning = '{}: ' if repo_name else ''\n",
    "    p = figure(x_range=x_groups, y_range=(0, y_max), plot_height=350, plot_width=750,  \n",
    "               title='{} {}'.format(title_beginning, title.format(description)))\n",
    "    \n",
    "    dodge_amount = 0.12\n",
    "    color_index = 0\n",
    "    x_offset = 50\n",
    "    for data_desc, input_df in data_dict.items():\n",
    "        driver_df = input_df.copy()\n",
    "        \n",
    "        driver_df[x_axis] = driver_df[x_axis].astype(str)\n",
    "        \n",
    "        groups = sorted(list(driver_df['merged_flag'].unique()))\n",
    "    \n",
    "        if repo_name:\n",
    "            driver_df = driver_df.loc[driver_df['repo_name'] == repo_name]\n",
    "            \n",
    "        len_merged = []\n",
    "        zeros = []\n",
    "        len_not_merged = []\n",
    "        totals = []\n",
    "        for x_group in x_groups:\n",
    "            print(x_group)\n",
    "            \n",
    "            len_merged_entry = len(driver_df.loc[(driver_df['merged_flag'] == 'Merged / Accepted') & (driver_df[x_axis] == x_group)])\n",
    "            totals += [len(driver_df.loc[(driver_df['merged_flag'] == 'Not Merged / Rejected') & (driver_df[x_axis] == x_group)]) + len_merged_entry]\n",
    "            len_not_merged += [len(driver_df.loc[(driver_df['merged_flag'] == 'Not Merged / Rejected') & (driver_df[x_axis] == x_group)])]\n",
    "            len_merged += [len_merged_entry]\n",
    "            zeros.append(0)\n",
    "            print(len_merged, len_not_merged)\n",
    "        \n",
    "        data = {'X': x_groups}\n",
    "        for group in groups:\n",
    "            data[group] = []\n",
    "            for x_group in x_groups:\n",
    "                data[group] += [len(driver_df.loc[(driver_df['merged_flag'] == group) & (driver_df[x_axis] == x_group)])]\n",
    "\n",
    "        data['len_merged'] = len_merged\n",
    "        data['len_not_merged'] = len_not_merged\n",
    "        data['totals'] = totals\n",
    "        data['zeros'] = zeros\n",
    "        source = ColumnDataSource(data)\n",
    "\n",
    "        stacked_bar = p.vbar_stack(groups, x=dodge('X', dodge_amount, range=p.x_range), width=0.2, source=source, color=colors[1:3], legend_label=[f\"{data_desc} \" + \"%s\" % x for x in groups])\n",
    "        # Data label for merged\n",
    "        p.add_layout(\n",
    "            LabelSet(x=dodge('X', dodge_amount, range=p.x_range), y='zeros', text='len_merged', y_offset=2, x_offset=x_offset,\n",
    "                  text_font_size=\"12pt\", text_color=colors[1:3][0],\n",
    "                  source=source, text_align='center')\n",
    "        )\n",
    "        # Data label for not merged\n",
    "        p.add_layout(\n",
    "            LabelSet(x=dodge('X', dodge_amount, range=p.x_range), y='totals', text='len_not_merged', y_offset=0, x_offset=x_offset,\n",
    "                  text_font_size=\"12pt\", text_color=colors[1:3][1],\n",
    "                  source=source, text_align='center')\n",
    "        )\n",
    "        # Data label for total\n",
    "        p.add_layout(\n",
    "            LabelSet(x=dodge('X', dodge_amount, range=p.x_range), y='totals', text='totals', y_offset=0, x_offset=0,\n",
    "                  text_font_size=\"12pt\", text_color='black',\n",
    "                  source=source, text_align='center')\n",
    "        )\n",
    "        dodge_amount *= -1\n",
    "        colors = colors[::-1]\n",
    "        x_offset *= -1\n",
    "\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.legend.location = \"top_left\"\n",
    "    p.axis.minor_tick_line_color = None\n",
    "    p.outline_line_color = None\n",
    "    p.yaxis.axis_label = 'Count of Pull Requests'\n",
    "    p.xaxis.axis_label = 'Repository' if x_axis == 'repo_name' else 'Year Closed' if x_axis == 'closed_year' else ''\n",
    "    \n",
    "    p.title.align = \"center\"\n",
    "    p.title.text_font_size = \"16px\"\n",
    "\n",
    "    p.xaxis.axis_label_text_font_size = \"16px\"\n",
    "    p.xaxis.major_label_text_font_size = \"16px\"\n",
    "\n",
    "    p.yaxis.axis_label_text_font_size = \"16px\"\n",
    "    p.yaxis.major_label_text_font_size = \"16px\"\n",
    "\n",
    "    p.outline_line_color = None\n",
    "    show(p, plot_width=1200, plot_height=1200)\n",
    "    if save_file:\n",
    "        repo_extension = 'All' if not repo_name else repo_name\n",
    "        export_png(p, filename=\"./images/v_stacked_bar_merged_status_count/stacked_bar_merged_status_count__facet_{}__{}_PRs__xaxis_{}__repo_{}.png\".format(facet, description, x_axis, repo_extension))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for repo_name in pr_all['repo_name'].unique():\n",
    "merged_ratio_vertical_grouped_bar({'All':pr_closed,'Slowest 20%':pr_slow20_not_merged.append(pr_slow20_merged,ignore_index=True)}, description=\"All Closed\", y_max=3800, x_axis='closed_year', save_file=False, title=\"Count of {} Pull Requests by Merged Status\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mean_response_times(input_df, time_unit='days', repo_name=None, y_axis='closed_year', description=\"All\", x_max=95, save_file=False, legend_position=(410, 10)):\n",
    "\n",
    "    output_notebook() # let bokeh show plot in jupyter cell output\n",
    "    \n",
    "    driver_df = input_df.copy()[['repo_name', 'merged_flag', y_axis, time_unit + '_to_first_response', time_unit + '_to_last_response', \n",
    "                                 time_unit + '_to_close']] # deep copy input data so we do not alter the external dataframe\n",
    "    \n",
    "    # filter by repo_name param\n",
    "    if repo_name:\n",
    "        driver_df = driver_df.loc[driver_df['repo_name'] == repo_name]\n",
    "\n",
    "    title_beginning = '' if not repo_name else '{}: '.format(repo_name)\n",
    "    p = figure(toolbar_location=None, y_range=sorted(driver_df[y_axis].unique()), x_range=(-2,x_max), plot_width=950, \n",
    "               plot_height=450,#75*len(driver_df[y_axis].unique()),\n",
    "               title=\"Mean Response Times for {} Pull Requests {}\".format(title_beginning, description))\n",
    "    \n",
    "    first_response_glyphs = []\n",
    "    last_response_glyphs = []\n",
    "    merged_days_to_close_glyphs = []\n",
    "    not_merged_days_to_close_glyphs = []\n",
    "\n",
    "    for y_value in driver_df[y_axis].unique():\n",
    "\n",
    "        y_merged_data = driver_df.loc[(driver_df[y_axis] == y_value) & (driver_df['merged_flag'] == 'Merged / Accepted')]\n",
    "        y_not_merged_data = driver_df.loc[(driver_df[y_axis] == y_value) & (driver_df['merged_flag'] == 'Not Merged / Rejected')]\n",
    "\n",
    "        y_merged_data[time_unit + '_to_first_response_mean'] = y_merged_data[time_unit + '_to_first_response'].mean().round(1) if len(y_merged_data) > 0 else 0.00\n",
    "        y_merged_data[time_unit + '_to_last_response_mean'] = y_merged_data[time_unit + '_to_last_response'].mean().round(1) if len(y_merged_data) > 0 else 0.00\n",
    "        y_merged_data[time_unit + '_to_close_mean'] = y_merged_data[time_unit + '_to_close'].mean().round(1) if len(y_merged_data) > 0 else 0.00\n",
    "        \n",
    "        y_not_merged_data[time_unit + '_to_first_response_mean'] = y_not_merged_data[time_unit + '_to_first_response'].mean().round(1) if len(y_not_merged_data) > 0 else 0.00\n",
    "        y_not_merged_data[time_unit + '_to_last_response_mean'] = y_not_merged_data[time_unit + '_to_last_response'].mean().round(1) if len(y_not_merged_data) > 0 else 0.00\n",
    "        y_not_merged_data[time_unit + '_to_close_mean'] = y_not_merged_data[time_unit + '_to_close'].mean().round(1) if len(y_not_merged_data) > 0 else 0.00\n",
    "\n",
    "        not_merged_source = ColumnDataSource(y_not_merged_data)\n",
    "        merged_source = ColumnDataSource(y_merged_data)\n",
    "\n",
    "        # mean PR length for merged\n",
    "        merged_days_to_close_glyph = p.hbar(y=dodge(y_axis, -0.1, range=p.y_range), left=0, right=time_unit + '_to_close_mean', height=0.04*len(driver_df[y_axis].unique()), \n",
    "                                     source=merged_source, fill_color=\"black\")#,legend_label=\"Mean Days to Close\",\n",
    "        merged_days_to_close_glyphs.append(merged_days_to_close_glyph)\n",
    "        # Data label \n",
    "        labels = LabelSet(x=time_unit + '_to_close_mean', y=dodge(y_axis, -0.1, range=p.y_range), text=time_unit + '_to_close_mean', y_offset=-8, x_offset=20, #34\n",
    "                  text_font_size=\"12pt\", text_color=\"black\",\n",
    "                  source=merged_source, text_align='center')\n",
    "        p.add_layout(labels)\n",
    "        # mean PR length For nonmerged\n",
    "        not_merged_days_to_close_glyph = p.hbar(y=dodge(y_axis, 0.1, range=p.y_range), left=0, right=time_unit + '_to_close_mean', \n",
    "                                     height=0.04*len(driver_df[y_axis].unique()), source=not_merged_source, fill_color=\"#e84d60\")#legend_label=\"Mean Days to Close\",\n",
    "        not_merged_days_to_close_glyphs.append(not_merged_days_to_close_glyph)\n",
    "        # Data label \n",
    "        labels = LabelSet(x=time_unit + '_to_close_mean', y=dodge(y_axis, 0.1, range=p.y_range), text=time_unit + '_to_close_mean', y_offset=-8, x_offset=44,\n",
    "                  text_font_size=\"12pt\", text_color=\"#e84d60\",\n",
    "                  source=not_merged_source, text_align='center')\n",
    "        p.add_layout(labels)\n",
    "\n",
    "        # mean time to first response\n",
    "        glyph = Rect(x=time_unit + '_to_first_response_mean', y=dodge(y_axis, -0.1, range=p.y_range), width=x_max/100, height=0.08*len(driver_df[y_axis].unique()), fill_color=colors[0])\n",
    "        first_response_glyph = p.add_glyph(merged_source, glyph)\n",
    "        first_response_glyphs.append(first_response_glyph)\n",
    "        # Data label \n",
    "        labels = LabelSet(x=time_unit + '_to_first_response_mean', y=dodge(y_axis, 0, range=p.y_range),text=time_unit + '_to_first_response_mean', y_offset=-45,#-75,\n",
    "                  text_font_size=\"12pt\", text_color=colors[0],\n",
    "                  source=merged_source, text_align='center')\n",
    "        p.add_layout(labels)\n",
    "        #for nonmerged\n",
    "        glyph = Rect(x=time_unit + '_to_first_response_mean', y=dodge(y_axis, 0.1, range=p.y_range), width=x_max/100, height=0.08*len(driver_df[y_axis].unique()), fill_color=colors[0])\n",
    "        first_response_glyph = p.add_glyph(not_merged_source, glyph)\n",
    "        first_response_glyphs.append(first_response_glyph)\n",
    "        # Data label \n",
    "        labels = LabelSet(x=time_unit + '_to_first_response_mean', y=dodge(y_axis, 0, range=p.y_range),text=time_unit + '_to_first_response_mean', y_offset=25,#55,\n",
    "                          text_font_size=\"12pt\", text_color=colors[0],\n",
    "                  source=not_merged_source, text_align='center')\n",
    "        p.add_layout(labels)\n",
    "\n",
    "        # mean time to last response\n",
    "        glyph = Rect(x=time_unit + '_to_last_response_mean', y=dodge(y_axis, -0.1, range=p.y_range), width=x_max/100, height=0.08*len(driver_df[y_axis].unique()), fill_color=colors[1])\n",
    "        last_response_glyph = p.add_glyph(merged_source, glyph)\n",
    "        last_response_glyphs.append(last_response_glyph)\n",
    "        # Data label \n",
    "        labels = LabelSet(x=time_unit + '_to_last_response_mean', y=dodge(y_axis, 0, range=p.y_range), text=time_unit + '_to_last_response_mean', y_offset=-45,#-75,\n",
    "                  text_font_size=\"12pt\", text_color=colors[1],\n",
    "                  source=merged_source, text_align='center')\n",
    "        p.add_layout(labels)\n",
    "        #for nonmerged\n",
    "        glyph = Rect(x=time_unit + '_to_last_response_mean', y=dodge(y_axis, 0.1, range=p.y_range), width=x_max/100, height=0.08*len(driver_df[y_axis].unique()), fill_color=colors[1])\n",
    "        last_response_glyph = p.add_glyph(not_merged_source, glyph)\n",
    "        last_response_glyphs.append(last_response_glyph)\n",
    "        # Data label \n",
    "        labels = LabelSet(x=time_unit + '_to_last_response_mean', y=dodge(y_axis, 0, range=p.y_range), text=time_unit + '_to_last_response_mean', y_offset=25,#55,\n",
    "                  text_font_size=\"12pt\", text_color=colors[1],\n",
    "                  source=not_merged_source, text_align='center')\n",
    "        p.add_layout(labels)\n",
    "            \n",
    "    p.title.align = \"center\"\n",
    "    p.title.text_font_size = \"16px\"\n",
    "\n",
    "    p.xaxis.axis_label = \"Days to Close\"\n",
    "    p.xaxis.axis_label_text_font_size = \"16px\"\n",
    "    p.xaxis.major_label_text_font_size = \"16px\"\n",
    "\n",
    "    p.yaxis.axis_label = \"Repository\" if y_axis == 'repo_name' else 'Year Closed' if y_axis == 'closed_year' else ''\n",
    "    p.yaxis.axis_label_text_font_size = \"16px\"\n",
    "    p.yaxis.major_label_text_font_size = \"16px\"\n",
    "    p.ygrid.grid_line_color = None\n",
    "    p.y_range.range_padding = 0.15\n",
    "\n",
    "    p.outline_line_color = None\n",
    "    p.toolbar.logo = None\n",
    "    p.toolbar_location = None\n",
    "\n",
    "    def add_legend(location, orientation, side):\n",
    "        legend = Legend(\n",
    "            items=[\n",
    "                (\"Mean Days to First Response\", first_response_glyphs),\n",
    "                (\"Mean Days to Last Response\", last_response_glyphs),\n",
    "                (\"Merged Mean Days to Close\", merged_days_to_close_glyphs),\n",
    "                (\"Not Merged Mean Days to Close\", not_merged_days_to_close_glyphs)\n",
    "            ],\n",
    "\n",
    "            location=location, \n",
    "            orientation=orientation,\n",
    "            border_line_color=\"black\"\n",
    "    #         title='Example Title'\n",
    "        )\n",
    "        p.add_layout(legend, side)\n",
    "\n",
    "#     add_legend((150, 50), \"horizontal\", \"center\")\n",
    "    add_legend(legend_position, \"vertical\", \"right\")\n",
    "    #viz_width=x_max/100, \n",
    "    #viz_height=0.08*len(driver_df[y_axis].unique()\n",
    "        \n",
    "    show(p, plot_width=1200, plot_height=1200)\n",
    "    \n",
    "    if save_file:\n",
    "        repo_extension = 'All' if not repo_name else repo_name\n",
    "        export_png(p, filename=\"./images/hbar_response_times/mean_response_times__{}_PRs__yaxis_{}__repo_{}.png\".format(description, y_axis, repo_extension))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for repo_name in pr_closed['repo_name'].unique():\n",
    "visualize_mean_response_times(pr_closed, description=\"All Closed\", y_axis='closed_year', x_max=244, save_file=False, legend_position='center')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mean_time_between_responses(data_dict, time_unit='Hours', x_axis='closed_yearmonth', description='', line_group='merged_flag',repo_name=None, y_max=800, y_axis='average_hours_between_responses', num_outliers_repo_map={}, save_file=False):\n",
    "    \n",
    "    output_notebook()\n",
    "    \n",
    "    p1 = figure(x_axis_type=\"datetime\", title=\"Mean {} Between Comments by Month Closed for {} Pull Requests\".format(time_unit, description), plot_width=950, x_range=(pr_all[x_axis].min(),pr_all[x_axis].max()),y_range=(0,y_max), plot_height=500)\n",
    "    colors = Category20[10][6:]\n",
    "    color_index = 0\n",
    "    \n",
    "    glyphs = []\n",
    "    \n",
    "    for data_desc, input_df in data_dict.items():\n",
    "    \n",
    "        driver_df = input_df.copy()\n",
    "\n",
    "        driver_df = remove_outliers(driver_df, y_axis, num_outliers_repo_map)\n",
    "\n",
    "        if repo_name:\n",
    "            driver_df = driver_df.loc[driver_df['repo_name'] == repo_name]\n",
    "\n",
    "        index = 0\n",
    "\n",
    "        driver_df_mean = driver_df.groupby(['repo_id', line_group, x_axis],as_index=False).mean()\n",
    "\n",
    "        title_ending = ''\n",
    "        if repo_name:\n",
    "            title_ending += ' for Repo: {}'.format(repo_name)\n",
    "        \n",
    "        for group_num, line_group_value in enumerate(driver_df[line_group].unique(), color_index):\n",
    "            glyphs.append(p1.line(driver_df_mean.loc[driver_df_mean[line_group] == line_group_value][x_axis], driver_df_mean.loc[driver_df_mean[line_group] == line_group_value][y_axis], color=colors[group_num]))\n",
    "            color_index += 1\n",
    "\n",
    "    for repo, num_outliers in num_outliers_repo_map.items():\n",
    "        if repo_name == repo:\n",
    "            p1.add_layout(Title(text=\"** {} outliers for {} were removed\".format(num_outliers, repo), align=\"center\"), \"below\")\n",
    "                \n",
    "    p1.grid.grid_line_alpha = 0.3\n",
    "    p1.xaxis.axis_label = 'Month Closed'\n",
    "    p1.xaxis.ticker.desired_num_ticks = 15\n",
    "    p1.yaxis.axis_label = 'Mean {} Between Responses'.format(time_unit)\n",
    "    p1.legend.location = \"top_left\"\n",
    "    \n",
    "    legend = Legend(\n",
    "        items=[\n",
    "            (\"All Not Merged / Rejected\", [glyphs[0]]),\n",
    "            (\"All Merged / Accepted\", [glyphs[1]]),\n",
    "            (\"Slowest 20% Not Merged / Rejected\", [glyphs[2]]),\n",
    "            (\"Slowest 20% Merged / Accepted\", [glyphs[3]])\n",
    "        ],\n",
    "\n",
    "        location='center_right', \n",
    "        orientation='vertical',\n",
    "        border_line_color=\"black\"\n",
    "    )\n",
    "    p1.add_layout(legend, 'right')\n",
    "\n",
    "    show(p1, plot_width=1200, plot_height=1200)\n",
    "\n",
    "    if save_file:\n",
    "        repo_extension = 'All' if not repo_name else repo_name\n",
    "        export_png(grid, filename=\"./images/line_mean_time_between_comments/line_mean_time_between_comments__{}_PRs__yaxis_{}__repo_{}.png\".format(description, y_axis, repo_extension))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for repo_name in pr_all['repo_name'].unique():\n",
    "visualize_mean_time_between_responses({'All':pr_closed,'Slowest 20%':pr_slow20_not_merged.append(pr_slow20_merged,ignore_index=True)}, line_group='merged_flag',description=\"All Closed\", x_axis='closed_yearmonth', save_file=False, y_max=190, time_unit='Days', y_axis='average_days_between_responses')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_time_to_first_comment(input_df, repo_name=None, x_axis='pr_created_at', y_max=300, y_axis='days_to_first_response', description=None, num_outliers_repo_map={}, group_by='merged_flag', same_scales=True, columns=2, legend_position='top_right', save_file=False):\n",
    "    output_notebook()#\n",
    "    \n",
    "    driver_df = input_df.copy()#\n",
    "    \n",
    "    #\n",
    "    if repo_name:\n",
    "        driver_df = driver_df.loc[driver_df['repo_name'] == repo_name]\n",
    "        \n",
    "    # \n",
    "    group_by_groups = sorted(driver_df[group_by].unique())\n",
    "\n",
    "    title_beginning = '{}: '.format(repo_name.capitalize()) if repo_name else ''\n",
    "    p = figure(x_range=(driver_df[x_axis].min() - datetime.timedelta(days=30), driver_df[x_axis].max() + datetime.timedelta(days=25)), \n",
    "               y_range=(0, y_max),#(driver_df[y_axis].min(), driver_df[y_axis].max()), \n",
    "               toolbar_location=None,\n",
    "               title='{}Days to First Response for {} Closed Pull Requests'.format(title_beginning, description), plot_width=900, plot_height=400, x_axis_type='datetime')\n",
    "\n",
    "    outliers = driver_df.loc[driver_df[y_axis] > y_max]\n",
    "    if len(outliers) > 0:\n",
    "        if repo_name:\n",
    "            p.add_layout(Title(text=\"** Outliers cut off at {} days: {} outlier(s) for {} were removed **\".format(y_max, len(outliers), repo_name), align=\"center\"), \"below\")\n",
    "        else:\n",
    "            p.add_layout(Title(text=\"** Outliers cut off at {} days: {} outlier(s) were removed **\".format(y_max, len(outliers)), align=\"center\"), \"below\")\n",
    "\n",
    "    for index, group_by_group in enumerate(group_by_groups):\n",
    "        p.scatter(x_axis, y_axis, color=colors[index], marker=\"square\", source=driver_df.loc[driver_df[group_by] == group_by_group], legend_label=group_by_group)\n",
    "\n",
    "    p.xaxis.axis_label = 'Date Closed' if x_axis == 'pr_closed_at' else 'Date Created' if x_axis == 'pr_created_at' else 'Date'\n",
    "    p.yaxis.axis_label = 'Days to First Response'\n",
    "    p.legend.location = legend_position\n",
    "    \n",
    "    p.title.align = \"center\"\n",
    "    p.title.text_font_size = \"15px\"\n",
    "\n",
    "    p.xaxis.axis_label_text_font_size = \"16px\"\n",
    "    p.xaxis.major_label_text_font_size = \"16px\"\n",
    "\n",
    "    p.yaxis.axis_label_text_font_size = \"16px\"\n",
    "    p.yaxis.major_label_text_font_size = \"16px\"\n",
    "            \n",
    "    show(p, plot_width=1200, plot_height=1200)\n",
    "    \n",
    "    if save_file:\n",
    "        repo_extension = 'All' if not repo_name else repo_name\n",
    "        export_png(p, filename=\"./images/first_comment_times/scatter_first_comment_times__{}_PRs__xaxis_{}__repo_{}.png\".format(description, x_axis, repo_extension))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for repo_name in pr_all['repo_name'].unique():\n",
    "visualize_time_to_first_comment(pr_closed, save_file=False, y_max=180, x_axis='pr_closed_at', group_by='merged_flag', y_axis='days_to_first_response', description=\"All\", legend_position='top_right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hex_to_RGB(hex):\n",
    "    ''' \"#FFFFFF\" -> [255,255,255] '''\n",
    "    # Pass 16 to the integer function for change of base\n",
    "    return [int(hex[i:i+2], 16) for i in range(1,6,2)]\n",
    "\n",
    "def color_dict(gradient):\n",
    "    ''' Takes in a list of RGB sub-lists and returns dictionary of\n",
    "    colors in RGB and hex form for use in a graphing function\n",
    "    defined later on '''\n",
    "    return {\"hex\":[RGB_to_hex(RGB) for RGB in gradient],\n",
    "      \"r\":[RGB[0] for RGB in gradient],\n",
    "      \"g\":[RGB[1] for RGB in gradient],\n",
    "      \"b\":[RGB[2] for RGB in gradient]}\n",
    "\n",
    "def RGB_to_hex(RGB):\n",
    "    ''' [255,255,255] -> \"#FFFFFF\" '''\n",
    "    # Components need to be integers for hex to make sense\n",
    "    RGB = [int(x) for x in RGB]\n",
    "    return \"#\"+\"\".join([\"0{0:x}\".format(v) if v < 16 else\n",
    "            \"{0:x}\".format(v) for v in RGB])\n",
    "\n",
    "def linear_gradient(start_hex, finish_hex=\"#FFFFFF\", n=10):\n",
    "    ''' returns a gradient list of (n) colors between\n",
    "    two hex colors. start_hex and finish_hex\n",
    "    should be the full six-digit color string,\n",
    "    inlcuding the number sign (\"#FFFFFF\") '''\n",
    "    # Starting and ending colors in RGB form\n",
    "    s = hex_to_RGB(start_hex)\n",
    "    f = hex_to_RGB(finish_hex)\n",
    "    # Initilize a list of the output colors with the starting color\n",
    "    RGB_list = [s]\n",
    "    # Calcuate a color at each evenly spaced value of t from 1 to n\n",
    "    for t in range(1, n):\n",
    "        # Interpolate RGB vector for color at the current value of t\n",
    "        curr_vector = [\n",
    "          int(s[j] + (float(t)/(n-1))*(f[j]-s[j]))\n",
    "          for j in range(3)\n",
    "        ]\n",
    "        # Add it to our list of output colors\n",
    "        RGB_list.append(curr_vector)\n",
    "\n",
    "    return color_dict(RGB_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import BasicTicker, ColorBar, LinearColorMapper, PrintfTickFormatter\n",
    "from bokeh.transform import transform\n",
    "\n",
    "def event_types_heat_map(input_df, include_comments=False, repo_name=None, x_axis='closed_year',facet=\"merged_flag\",columns=2, x_max=1100, same_scales=True, save_file=False, y_axis='repo_name', description=\"All\", title=\"Average Pull Request Comments by Merged Status\"):\n",
    "    colors = linear_gradient('#f5f5dc', '#fff44f', 150)['hex']\n",
    "\n",
    "    driver_df = input_df.copy()\n",
    "    driver_df[x_axis] = driver_df[x_axis].astype(str)\n",
    "    \n",
    "    if repo_name:\n",
    "        driver_df = driver_df.loc[driver_df['repo_name'] == repo_name]\n",
    "    \n",
    "    if facet == 'closed_year' or y_axis == 'closed_year':\n",
    "        driver_df['closed_year'] = driver_df['closed_year'].astype(int).astype(str)\n",
    "        \n",
    "    optional_comments = ['comment_count'] if include_comments else []\n",
    "    driver_df = driver_df[['repo_id', 'repo_name',x_axis, 'assigned_count',\n",
    "          'review_requested_count',\n",
    "          'labeled_count',\n",
    "          'subscribed_count',\n",
    "          'mentioned_count',\n",
    "          'referenced_count',\n",
    "          'closed_count',\n",
    "          'head_ref_force_pushed_count',\n",
    "          'merged_count',\n",
    "          'milestoned_count',\n",
    "          'unlabeled_count',\n",
    "          'head_ref_deleted_count', facet ] + optional_comments]\n",
    "    y_groups = ['assigned_count',\n",
    "          'review_requested_count',\n",
    "          'labeled_count',\n",
    "          'subscribed_count',\n",
    "          'mentioned_count',\n",
    "          'referenced_count',\n",
    "          'closed_count',\n",
    "          'head_ref_force_pushed_count',\n",
    "          'merged_count',\n",
    "          'milestoned_count',\n",
    "          'unlabeled_count',\n",
    "          'head_ref_deleted_count'] + optional_comments\n",
    "    output_notebook()\n",
    "    optional_group_comments = ['comment'] if include_comments else []\n",
    "#     y_groups = ['subscribed', 'mentioned', 'labeled', 'review_requested', 'head_ref_force_pushed', 'referenced', 'closed', 'merged', 'unlabeled', 'head_ref_deleted', 'milestoned', 'assigned'] + optional_group_comments\n",
    "\n",
    "    x_groups = sorted(list(driver_df[x_axis].unique()))\n",
    "    \n",
    "    grid_array = []\n",
    "    grid_row = []  \n",
    "    \n",
    "    for index, facet_group in enumerate(sorted(driver_df[facet].unique())):\n",
    "        print(facet_group)\n",
    "        \n",
    "        facet_data = driver_df.loc[driver_df[facet] == facet_group]\n",
    "        print(len(facet_data))\n",
    "#         display(facet_data.sort_values('merged_count', ascending=False).head(50))\n",
    "        driver_df_mean = facet_data.groupby(['repo_id', 'repo_name', x_axis], as_index=False).mean().round(1)\n",
    "#         data = {'Y' : y_groups}\n",
    "#         for group in y_groups:\n",
    "#             data[group] = driver_df_mean[group].tolist()\n",
    "    \n",
    "        title_ending = 'Created in {}'.format(facet_group) if facet == 'created_year' else 'Closed in {}'.format(facet_group) if facet == 'closed_year' else 'for Repo: {}'.format(repo_name) if repo_name else facet_group\n",
    "        p = figure(y_range=y_groups, plot_height=700, plot_width=900, x_range=x_groups, \n",
    "                   title='{} {}'.format(title.format(description), title_ending))\n",
    "        \n",
    "        for y_group in y_groups:\n",
    "            driver_df_mean['field'] = y_group\n",
    "            source = ColumnDataSource(driver_df_mean)\n",
    "            mapper = LinearColorMapper(palette=colors, low=driver_df_mean[y_group].min(), high=driver_df_mean[y_group].max())\n",
    "            \n",
    "            p.rect(y='field', x=x_axis, width=1, height=1, source=source,\n",
    "                   line_color=None, fill_color=transform(y_group, mapper))\n",
    "            # Data label \n",
    "            labels = LabelSet(x=x_axis, y='field', text=y_group, y_offset=-8,\n",
    "                      text_font_size=\"12pt\", text_color='black',\n",
    "                      source=source, text_align='center')\n",
    "            p.add_layout(labels)\n",
    "\n",
    "            color_bar = ColorBar(color_mapper=mapper, location=(0, 0),\n",
    "                                 ticker=BasicTicker(desired_num_ticks=9),\n",
    "                                 formatter=PrintfTickFormatter(format=\"%d\"))\n",
    "#         p.add_layout(color_bar, 'right')\n",
    "        \n",
    "        p.y_range.range_padding = 0.1\n",
    "        p.ygrid.grid_line_color = None\n",
    "        \n",
    "        p.legend.location = \"bottom_right\"\n",
    "        p.axis.minor_tick_line_color = None\n",
    "        p.outline_line_color = None\n",
    "        \n",
    "        p.xaxis.axis_label = 'Year Closed'\n",
    "        p.yaxis.axis_label = 'Event Type'\n",
    "        \n",
    "        p.title.align = \"center\"\n",
    "        p.title.text_font_size = \"12px\"\n",
    "        \n",
    "        p.xaxis.axis_label_text_font_size = \"16px\"\n",
    "        p.xaxis.major_label_text_font_size = \"16px\"\n",
    "        \n",
    "        p.yaxis.axis_label_text_font_size = \"16px\"\n",
    "        p.yaxis.major_label_text_font_size = \"16px\"\n",
    "                \n",
    "        grid_row.append(p)\n",
    "        if index % columns == columns - 1:\n",
    "            grid_array.append(grid_row)\n",
    "            grid_row = []\n",
    "    grid = gridplot(grid_array)\n",
    "    show(grid, plot_width=1200, plot_height=1200)\n",
    "    if save_file:\n",
    "        comments_included = 'comments_included' if include_comments else 'comments_not_included'\n",
    "        repo_extension = 'All' if not repo_name else repo_name\n",
    "        export_png(grid, filename=\"./images/h_stacked_bar_mean_event_types/mean_event_types__facet_{}__{}_PRs__yaxis_{}__{}__repo_{}.png\".format(facet, description, y_axis, comments_included, repo_extension))\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for repo_name in pr_all['repo_name'].unique():\n",
    "event_types_heat_map(pr_closed, facet=\"merged_flag\", save_file=False, x_axis='closed_year', description=\"All Closed\", include_comments=True, title=\"Average Pull Request Event Types for {} Pull Requests\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure\n",
    "\n",
    "red_green_gradient = linear_gradient('#0080FF', '#DC143C', 150)['hex']\n",
    "    #32CD32\n",
    "def heat_map(input_df, repo_name=None,x_axis='repo_id', group_by='merged_flag', y_axis='closed_yearmonth', heat_max=300, save_file=False, same_scales=True, description=\"All\", heat_field='days_to_first_response', columns=2):\n",
    "    \n",
    "    output_notebook()\n",
    "    \n",
    "    driver_df = input_df.copy()[['repo_id', y_axis, group_by, x_axis, heat_field]]\n",
    "    \n",
    "    if repo_name:\n",
    "        driver_df = driver_df.loc[driver_df['repo_name'] == repo_name]\n",
    "        \n",
    "    driver_df[y_axis] = driver_df[y_axis].astype(str)\n",
    "    \n",
    "    # add new group by + xaxis column \n",
    "    driver_df['grouped_x'] = driver_df[x_axis] + ' - ' + driver_df[group_by]\n",
    "\n",
    "    driver_df_mean = driver_df.groupby(['grouped_x', y_axis], as_index=False).mean()\n",
    "\n",
    "    colors = red_green_gradient\n",
    "    y_groups = driver_df_mean[y_axis].unique()\n",
    "    x_groups = sorted(driver_df[x_axis].unique())\n",
    "    grouped_x_groups = sorted(driver_df_mean['grouped_x'].unique())\n",
    "    print(grouped_x_groups)\n",
    "    \n",
    "    mapper = LinearColorMapper(palette=colors, low=driver_df_mean[heat_field].min(), high=heat_max)#driver_df_mean[heat_field].max())\n",
    "    \n",
    "    source = ColumnDataSource(driver_df_mean)\n",
    "\n",
    "    p = figure(plot_width=1100, plot_height=300, title=\"Mean Duration (Days) {} Pull Requests\".format(description),\n",
    "               y_range=grouped_x_groups[::-1], x_range=y_groups,\n",
    "               toolbar_location=None, tools=\"\")#, x_axis_location=\"above\")\n",
    "\n",
    "    for x_group in x_groups:\n",
    "        outliers = driver_df_mean.loc[(driver_df_mean[heat_field] > heat_max) & (driver_df_mean['grouped_x'].str.contains(x_group))]\n",
    "        print(outliers)\n",
    "        if len(outliers) > 0:\n",
    "            p.add_layout(Title(text=\"** Outliers capped at {} days: {} outlier(s) for {} were capped at {} **\".format(heat_max, len(outliers), x_group, heat_max), align=\"center\"), \"below\")\n",
    "\n",
    "    p.rect(x=y_axis, y='grouped_x', width=1, height=1, source=source,\n",
    "           line_color=None, fill_color=transform(heat_field, mapper))\n",
    "\n",
    "    color_bar = ColorBar(color_mapper=mapper, location=(0, 0),\n",
    "                         ticker=BasicTicker(desired_num_ticks=9),\n",
    "                         formatter=PrintfTickFormatter(format=\"%d\"))\n",
    "\n",
    "    p.add_layout(color_bar, 'right')\n",
    "    \n",
    "    p.title.align = \"center\"\n",
    "    p.title.text_font_size = \"14px\"\n",
    "\n",
    "    p.axis.axis_line_color = None\n",
    "    p.axis.major_tick_line_color = None\n",
    "    p.axis.major_label_text_font_size = \"11pt\"\n",
    "    p.axis.major_label_standoff = 0\n",
    "    p.xaxis.major_label_orientation = 1.0\n",
    "    p.xaxis.axis_label = 'Month Closed' if y_axis[0:6] == 'closed' else 'Date Created' if y_axis[0:7] == 'created' else 'Repository' if y_axis == 'repo_name' else ''\n",
    "#     p.yaxis.axis_label = 'Merged Status'\n",
    "        \n",
    "    show(p)\n",
    "    \n",
    "    if save_file:\n",
    "        repo_extension = 'All' if not repo_name else repo_name\n",
    "        export_png(p, filename=\"./images/heat_map_pr_duration_merged_status/heat_map_duration_by_merged_status__{}_PRs__yaxis_{}__repo_{}.png\".format(description, y_axis, repo_extension))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_map(pr_closed, x_axis='repo_name', group_by='merged_flag', y_axis='closed_yearmonth', save_file=False, description=\"All Closed\", heat_max=25, heat_field='days_to_first_response')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
